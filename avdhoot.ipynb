{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   File_Number  KMeans_Cluster  DBSCAN_Cluster  Agglomerative_Cluster\n",
      "0            1               1              -1                      2\n",
      "1            2               1              -1                      2\n",
      "2            3               0              -1                      1\n",
      "3            4               2              -1                      3\n",
      "4            5               3              -1                      0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder containing your MFCC files\n",
    "folder_path = '/home/avdhoot/third_sem/ds203/project/MFCC-files-v2'  # Update with your path\n",
    "\n",
    "# Feature extraction for all 20 MFCCs + additional statistical features\n",
    "features_list = []\n",
    "\n",
    "for file in range(1, 117):\n",
    "    # Naming format for MFCC files (like \"1.csv\", \"2.csv\", ...)\n",
    "    filename = os.path.join(folder_path, f\"{file}.csv\")\n",
    "    \n",
    "    # Load MFCCs from CSV\n",
    "    mfcc_df = pd.read_csv(filename, header=None)\n",
    "    mfcc_df = mfcc_df.transpose()\n",
    "    \n",
    "    # Extract the first 20 MFCCs\n",
    "    mfcc_20 = mfcc_df.values[:, :20]\n",
    "    \n",
    "    # Calculate statistical features for each of the first 5 MFCCs\n",
    "    file_features = []\n",
    "    for i in range(5):\n",
    "        coeff = mfcc_20[:, i]\n",
    "        file_features.extend([\n",
    "            np.mean(coeff),           # Mean\n",
    "            np.var(coeff),            # Variance\n",
    "            pd.Series(coeff).skew(),  # Skewness\n",
    "            pd.Series(coeff).kurt(),  # Kurtosis\n",
    "            np.sqrt(np.mean(coeff**2)),  # Root Mean Square\n",
    "            np.ptp(coeff),            # Peak-to-peak range\n",
    "            np.std(coeff)             # Standard deviation\n",
    "        ])\n",
    "    \n",
    "    # Append the features for this file to the list\n",
    "    features_list.append(file_features)\n",
    "\n",
    "# Convert the list of features to a DataFrame\n",
    "columns = []\n",
    "for i in range(1, 6):\n",
    "    columns += [f'mean_mfcc{i}', f'var_mfcc{i}', f'skew_mfcc{i}', f'kurt_mfcc{i}', \n",
    "                f'rms_mfcc{i}', f'ptp_mfcc{i}', f'std_mfcc{i}']\n",
    "features_df = pd.DataFrame(features_list, columns=columns)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca_features = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Apply different clustering algorithms\n",
    "cluster_results = {}\n",
    "\n",
    "# 1. KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(pca_features)\n",
    "cluster_results['KMeans'] = kmeans_labels\n",
    "\n",
    "# 2. DBSCAN Clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(pca_features)\n",
    "cluster_results['DBSCAN'] = dbscan_labels\n",
    "\n",
    "# 3. Agglomerative Clustering\n",
    "agglo = AgglomerativeClustering(n_clusters=6)\n",
    "agglo_labels = agglo.fit_predict(pca_features)\n",
    "cluster_results['Agglomerative'] = agglo_labels\n",
    "\n",
    "# Prepare data for saving, with file numbers and their cluster assignments\n",
    "cluster_data = pd.DataFrame({\n",
    "    'File_Number': list(range(1, 117)),\n",
    "    'KMeans_Cluster': cluster_results['KMeans'],\n",
    "    'DBSCAN_Cluster': cluster_results['DBSCAN'],\n",
    "    'Agglomerative_Cluster': cluster_results['Agglomerative']\n",
    "})\n",
    "\n",
    "# Save to a CSV for easy review\n",
    "# cluster_data.to_csv('clustering_results_with_pca.csv', index=False)\n",
    "\n",
    "print(cluster_data.head())  # Check the first few entries of the clustering results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 2)\n"
     ]
    }
   ],
   "source": [
    "label_df=pd.read_csv(\"/home/avdhoot/Documents/labels.csv\")\n",
    "print(label_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     File_Number  KMeans_Cluster  DBSCAN_Cluster  Agglomerative_Cluster\n",
      "0              1               1              -1                      2\n",
      "1              2               1              -1                      2\n",
      "2              3               0              -1                      1\n",
      "3              4               2              -1                      3\n",
      "4              5               3              -1                      0\n",
      "..           ...             ...             ...                    ...\n",
      "111          112               2              -1                      3\n",
      "112          113               2              -1                      3\n",
      "113          114               0              -1                      1\n",
      "114          115               4              -1                      3\n",
      "115          116               4              -1                      4\n",
      "\n",
      "[116 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 66.67%\n",
      "Predictions saved to 'unlabeled_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(audio_file, n_mfcc=20):\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    features = []\n",
    "\n",
    "    # Compute statistical features for first 5 MFCC coefficients\n",
    "    for i in range(5):\n",
    "        coeff = mfcc[i, :]\n",
    "        features.extend([\n",
    "            np.mean(coeff), \n",
    "            np.var(coeff), \n",
    "            pd.Series(coeff).skew(), \n",
    "            pd.Series(coeff).kurtosis(), \n",
    "            np.sqrt(np.mean(coeff ** 2)),  # RMS\n",
    "            np.ptp(coeff)  # Peak-to-peak ratio\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "# Paths to labeled folders and their labels\n",
    "data_dir = \"/home/avdhoot/third_sem/ds203/project/param_songs\"  # Replace with the actual path\n",
    "categories = {\n",
    "    \"asha bhosale\": 0,\n",
    "    \"kishore kumar\": 1,\n",
    "    \"marathi bhavgeet\": 2,\n",
    "    \"marathi lavni\": 3,\n",
    "    \"mj\": 4,\n",
    "    \"na\": 5\n",
    "}\n",
    "\n",
    "# Collect features and labels\n",
    "labeled_features = []\n",
    "labeled_labels = []\n",
    "\n",
    "for category, label in categories.items():\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    for audio_file in os.listdir(category_path):\n",
    "        if audio_file.endswith(\".mp3\") or audio_file.endswith(\".wav\"):  # Adjust for your audio formats\n",
    "            file_path = os.path.join(category_path, audio_file)\n",
    "            features = extract_features(file_path)\n",
    "            labeled_features.append(features)\n",
    "            labeled_labels.append(label)\n",
    "\n",
    "# Convert to DataFrame for labeled data\n",
    "labeled_df = pd.DataFrame(labeled_features)\n",
    "labeled_df['label'] = labeled_labels\n",
    "\n",
    "# Load unlabeled MFCC files and extract same features\n",
    "unlabeled_dir = \"/home/avdhoot/third_sem/ds203/project/MFCC-files-v2\"  # Replace with actual path\n",
    "unlabeled_features = []\n",
    "\n",
    "for i in range(1, 117):\n",
    "    file_path = os.path.join(unlabeled_dir, f\"{i}.csv\")\n",
    "    mfcc_df = pd.read_csv(file_path, header=None).transpose()\n",
    "\n",
    "    # Compute features for first 5 MFCCs as before\n",
    "    file_features = []\n",
    "    for j in range(5):\n",
    "        coeff = mfcc_df.iloc[:, j].values\n",
    "        file_features.extend([\n",
    "            np.mean(coeff), \n",
    "            np.var(coeff), \n",
    "            pd.Series(coeff).skew(), \n",
    "            pd.Series(coeff).kurtosis(), \n",
    "            np.sqrt(np.mean(coeff ** 2)),  # RMS\n",
    "            np.ptp(coeff)  # Peak-to-peak ratio\n",
    "        ])\n",
    "    unlabeled_features.append(file_features)\n",
    "\n",
    "# Convert to DataFrame for unlabeled data\n",
    "unlabeled_df = pd.DataFrame(unlabeled_features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_labeled = scaler.fit_transform(labeled_df.drop(columns=['label']))\n",
    "X_unlabeled = scaler.transform(unlabeled_df)\n",
    "y_labeled = labeled_df['label'].values\n",
    "\n",
    "# Split labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='linear', C=1, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict labels for unlabeled data\n",
    "unlabeled_predictions = svm.predict(X_unlabeled)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "unlabeled_results = pd.DataFrame({\n",
    "    'File_Number': list(range(1, 117)),\n",
    "    'Predicted_Label': unlabeled_predictions\n",
    "})\n",
    "unlabeled_results.to_csv(\"unlabeled_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'unlabeled_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     File_Number  Predicted_Label\n",
      "0              1                2\n",
      "1              2                2\n",
      "2              3                4\n",
      "3              4                3\n",
      "4              5                2\n",
      "..           ...              ...\n",
      "111          112                3\n",
      "112          113                3\n",
      "113          114                4\n",
      "114          115                2\n",
      "115          116                2\n",
      "\n",
      "[116 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df5=pd.read_csv(\"unlabeled_predictions.csv\")\n",
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 66.67%\n",
      "Predictions saved to 'unlabeled_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_features(audio_file, n_mfcc=20):\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    features = []\n",
    "\n",
    "    # Compute statistical features for first 5 MFCC coefficients\n",
    "    for i in range(5):\n",
    "        coeff = mfcc[i, :]\n",
    "        features.extend([\n",
    "            np.mean(coeff), \n",
    "            np.var(coeff), \n",
    "            pd.Series(coeff).skew(), \n",
    "            pd.Series(coeff).kurtosis(), \n",
    "            np.sqrt(np.mean(coeff ** 2)),  # RMS\n",
    "            np.ptp(coeff)  # Peak-to-peak ratio\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "# Paths to labeled folders and their labels\n",
    "data_dir = \"/home/avdhoot/third_sem/ds203/project/param_songs\"  # Replace with the actual path\n",
    "categories = {\n",
    "    \"asha bhosale\": 0,\n",
    "    \"kishore kumar\": 1,\n",
    "    \"marathi bhavgeet\": 2,\n",
    "    \"marathi lavni\": 3,\n",
    "    \"mj\": 4,\n",
    "    \"na\": 5\n",
    "}\n",
    "\n",
    "# Collect features and labels\n",
    "labeled_features = []\n",
    "labeled_labels = []\n",
    "\n",
    "for category, label in categories.items():\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    for audio_file in os.listdir(category_path):\n",
    "        if audio_file.endswith(\".mp3\") or audio_file.endswith(\".wav\"):  # Adjust for your audio formats\n",
    "            file_path = os.path.join(category_path, audio_file)\n",
    "            features = extract_features(file_path)\n",
    "            labeled_features.append(features)\n",
    "            labeled_labels.append(label)\n",
    "\n",
    "# Convert to DataFrame for labeled data\n",
    "labeled_df = pd.DataFrame(labeled_features)\n",
    "labeled_df['label'] = labeled_labels\n",
    "\n",
    "# Load unlabeled MFCC files and extract same features\n",
    "unlabeled_dir = \"/home/avdhoot/third_sem/ds203/project/MFCC-files-v2\"  # Replace with actual path\n",
    "unlabeled_features = []\n",
    "\n",
    "for i in range(1, 117):\n",
    "    file_path = os.path.join(unlabeled_dir, f\"{i}.csv\")\n",
    "    mfcc_df = pd.read_csv(file_path, header=None).transpose()\n",
    "\n",
    "    # Compute features for first 5 MFCCs as before\n",
    "    file_features = []\n",
    "    for j in range(5):\n",
    "        coeff = mfcc_df.iloc[:, j].values\n",
    "        file_features.extend([\n",
    "            np.mean(coeff), \n",
    "            np.var(coeff), \n",
    "            pd.Series(coeff).skew(), \n",
    "            pd.Series(coeff).kurtosis(), \n",
    "            np.sqrt(np.mean(coeff ** 2)),  # RMS\n",
    "            np.ptp(coeff)  # Peak-to-peak ratio\n",
    "        ])\n",
    "    unlabeled_features.append(file_features)\n",
    "\n",
    "# Convert to DataFrame for unlabeled data\n",
    "unlabeled_df = pd.DataFrame(unlabeled_features)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_labeled = scaler.fit_transform(labeled_df.drop(columns=['label']))\n",
    "X_unlabeled = scaler.transform(unlabeled_df)\n",
    "y_labeled = labeled_df['label'].values\n",
    "\n",
    "# Split labeled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "svm = SVC(kernel='rbf', C=1, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model on test set\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict labels for unlabeled data\n",
    "unlabeled_predictions = svm.predict(X_unlabeled)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "unlabeled_results = pd.DataFrame({\n",
    "    'File_Number': list(range(1, 117)),\n",
    "    'Predicted_Label': unlabeled_predictions\n",
    "})\n",
    "unlabeled_results.to_csv(\"unlabeled_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'unlabeled_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 2)\n"
     ]
    }
   ],
   "source": [
    "df5=pd.read_csv(\"/home/avdhoot/Documents/labels.csv\")\n",
    "print(df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m validation_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(validation_labels)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Standardize validation data\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m validation_features \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m validation_features \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(validation_features)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1035\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1030\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1032\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m             )\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1041\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paths to directories\n",
    "labeled_data_folder = '/home/avdhoot/third_sem/ds203/project/param_songs'  # Replace with actual path\n",
    "unlabeled_mfcc_folder = '/home/avdhoot/third_sem/ds203/project/MFCC-files-v2'  # Replace with actual path\n",
    "\n",
    "# Categories and labels for the labeled dataset\n",
    "categories = {\n",
    "    'na': 0,\n",
    "    'marathi lavni': 1,\n",
    "    'marathi bhavgeet': 2,\n",
    "    'asha bhosale': 3,\n",
    "    'mj': 4,\n",
    "    'kishore kumar': 5\n",
    "}\n",
    "\n",
    "# Function to extract features from audio files\n",
    "def extract_audio_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    features = []\n",
    "    for i in range(5):  # First 5 MFCCs\n",
    "        features.extend([np.mean(mfcc[i]), np.var(mfcc[i]), np.std(mfcc[i]), \n",
    "                         pd.Series(mfcc[i]).skew(), pd.Series(mfcc[i]).kurt(), \n",
    "                         np.sqrt(np.mean(mfcc[i]**2)), np.ptp(mfcc[i])])\n",
    "    return features\n",
    "\n",
    "# Prepare labeled training data\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for category, label in categories.items():\n",
    "    category_folder = os.path.join(labeled_data_folder, category)\n",
    "    for file_name in os.listdir(category_folder):\n",
    "        if file_name.endswith('.mp3'):\n",
    "            file_path = os.path.join(category_folder, file_name)\n",
    "            features = extract_audio_features(file_path)\n",
    "            train_features.append(features)\n",
    "            train_labels.append(label)\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "# Reduce dimensionality with PCA if desired\n",
    "pca = PCA(n_components=20)\n",
    "train_features = pca.fit_transform(train_features)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='rbf')  # RBF kernel can capture non-linear relations\n",
    "svm_model.fit(train_features, train_labels)\n",
    "\n",
    "# Prepare validation data\n",
    "df5 = pd.DataFrame()  # Assuming df5 is loaded with file numbers and labels\n",
    "validation_features = []\n",
    "validation_labels = []\n",
    "\n",
    "for _, row in df5.iterrows():\n",
    "    file_number = int(row[0])\n",
    "    label = int(row[1])\n",
    "    file_path = os.path.join(unlabeled_mfcc_folder, f\"{file_number}.csv\")\n",
    "    \n",
    "    # Load MFCCs directly from the CSV files\n",
    "    mfcc_df = pd.read_csv(file_path, header=None)\n",
    "    mfcc_df = mfcc_df.transpose()\n",
    "    \n",
    "    # Extract features from the first 5 MFCCs\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        mfcc_coeff = mfcc_df[i].values\n",
    "        features.extend([np.mean(mfcc_coeff), np.var(mfcc_coeff), np.std(mfcc_coeff),\n",
    "                         pd.Series(mfcc_coeff).skew(), pd.Series(mfcc_coeff).kurt(),\n",
    "                         np.sqrt(np.mean(mfcc_coeff**2)), np.ptp(mfcc_coeff)])\n",
    "    \n",
    "    validation_features.append(features)\n",
    "    validation_labels.append(label)\n",
    "\n",
    "validation_features = np.array(validation_features)\n",
    "validation_labels = np.array(validation_labels)\n",
    "\n",
    "# Standardize validation data\n",
    "validation_features = scaler.transform(validation_features)\n",
    "validation_features = pca.transform(validation_features)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = svm_model.predict(validation_features)\n",
    "accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(27, 20)\n"
     ]
    }
   ],
   "source": [
    "print(validation_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid validation data available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Paths to directories\n",
    "labeled_data_folder = '/home/avdhoot/third_sem/ds203/project/param_songs'  # Replace with actual path\n",
    "unlabeled_mfcc_folder = '/home/avdhoot/third_sem/ds203/project/data/MFCC-files-v2'  # Replace with actual path\n",
    "\n",
    "# Categories and labels for the labeled dataset\n",
    "categories = {\n",
    "    'na': 0,\n",
    "    'marathi lavni': 1,\n",
    "    'marathi bhavgeet': 2,\n",
    "    'asha bhosale': 3,\n",
    "    'mj': 4,\n",
    "    'kishore kumar': 5\n",
    "}\n",
    "\n",
    "# Function to extract features from audio files\n",
    "def extract_audio_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    features = []\n",
    "    for i in range(5):  # First 5 MFCCs\n",
    "        features.extend([np.mean(mfcc[i]), np.var(mfcc[i]), np.std(mfcc[i]), \n",
    "                         pd.Series(mfcc[i]).skew(), pd.Series(mfcc[i]).kurt(), \n",
    "                         np.sqrt(np.mean(mfcc[i]**2)), np.ptp(mfcc[i])])\n",
    "    return features\n",
    "\n",
    "# Prepare labeled training data\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for category, label in categories.items():\n",
    "    category_folder = os.path.join(labeled_data_folder, category)\n",
    "    for file_name in os.listdir(category_folder):\n",
    "        if file_name.endswith('.mp3'):\n",
    "            file_path = os.path.join(category_folder, file_name)\n",
    "            features = extract_audio_features(file_path)\n",
    "            train_features.append(features)\n",
    "            train_labels.append(label)\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "# Reduce dimensionality with PCA if desired\n",
    "pca = PCA(n_components=20)\n",
    "train_features = pca.fit_transform(train_features)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='rbf')  # RBF kernel can capture non-linear relations\n",
    "svm_model.fit(train_features, train_labels)\n",
    "\n",
    "# Prepare validation data\n",
    "validation_features = []\n",
    "validation_labels = []\n",
    "\n",
    "for _, row in df5.iterrows():\n",
    "    file_number = int(row[0])\n",
    "    label = int(row[1])\n",
    "    file_path = os.path.join(unlabeled_mfcc_folder, f\"{file_number}.csv\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File {file_path} not found, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Load MFCCs directly from the CSV files\n",
    "    mfcc_df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # Check if the file is empty or has fewer rows than expected\n",
    "    if mfcc_df.empty or mfcc_df.shape[0] < 20:\n",
    "        print(f\"File {file_path} is empty or has insufficient rows, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Transpose to have frames on columns and coefficients in rows\n",
    "    mfcc_df = mfcc_df.transpose()\n",
    "    \n",
    "    # Extract features from the first 5 MFCCs\n",
    "    features = []\n",
    "    for i in range(5):\n",
    "        mfcc_coeff = mfcc_df[i].values\n",
    "        features.extend([np.mean(mfcc_coeff), np.var(mfcc_coeff), np.std(mfcc_coeff),\n",
    "                         pd.Series(mfcc_coeff).skew(), pd.Series(mfcc_coeff).kurt(),\n",
    "                         np.sqrt(np.mean(mfcc_coeff**2)), np.ptp(mfcc_coeff)])\n",
    "    \n",
    "    validation_features.append(features)\n",
    "    validation_labels.append(label)\n",
    "\n",
    "# Convert to NumPy array if there are any valid entries\n",
    "if validation_features:\n",
    "    validation_features = np.array(validation_features)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "    # Standardize validation data\n",
    "    validation_features = scaler.transform(validation_features)\n",
    "    validation_features = pca.transform(validation_features)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions = svm_model.predict(validation_features)\n",
    "    accuracy = accuracy_score(validation_labels, predictions)\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid validation data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standalone files have been copied to 'kk'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "neural_data_folder = '/home/avdhoot/third_sem/ds203/project/neural_data'  # Replace with your actual path\n",
    "new_folder = os.path.join(neural_data_folder, 'kk')\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "# Loop through items in the neural data folder\n",
    "for item in os.listdir(neural_data_folder):\n",
    "    item_path = os.path.join(neural_data_folder, item)\n",
    "    \n",
    "    # Check if it's a standalone file (not a folder)\n",
    "    if os.path.isfile(item_path):\n",
    "        shutil.copy(item_path, new_folder)\n",
    "\n",
    "print(\"Standalone files have been copied to 'kk'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: Ban Ke Gulgule - Apna Haath Jagnnath 128 Kbps.mp3\n",
      "Deleted file: Bachna Ae Hasinon Lo Main Aa Gaya - Hum Kisise Kum Naheen 128 Kbps.mp3\n",
      "Deleted file: Badi Sooni Sooni Hai Zindagi - Mili 128 Kbps.mp3\n",
      "Deleted file: Apni To Jaise Taise - Laawaris 128 Kbps.mp3\n",
      "Deleted file: Chahiye Thoda Pyar - Lahu Ke Do Rang 128 Kbps.mp3\n",
      "Deleted file: Aapke Anurodh Pe - Anurodh 128 Kbps.mp3\n",
      "Deleted file: Chal Sapnon Ke Shahar Mein - Deewangi 1976 128 Kbps.mp3\n",
      "Deleted file: Ae-Oh-Aa-Zara-Mudke-From-Disco-Dancer-Kishore-Kumar.mp3\n",
      "Deleted file: Aaye Tum Yaad Mujhe - Mili 128 Kbps.mp3\n",
      "Deleted file: Aaj-Ei-Dintake-Kishore-Kumar.mp3\n",
      "Deleted file: Andheri Raaton Mein - Shahenshah 128 Kbps.mp3\n",
      "Deleted file: Chal Chal Chal Mere Saathi - Haathi Mere Saathi (1971) 128 Kbps.mp3\n",
      "Deleted file: Chala Jata Hoon - Mere Jeevan Saathi 128 Kbps.mp3\n",
      "Deleted file: Chahe-Koi-Khush-Ho-Kishore-Kumar.mp3\n",
      "Deleted file: Aise Na Mujhe - Darling Darling 128 Kbps.mp3\n",
      "Deleted file: Aaj Unse Pehli Mulaqat Hogi - Paraya Dhan 128 Kbps.mp3\n",
      "Deleted file: Bambai Ne Paida Kiya - Jaan Ki Baazi 128 Kbps.mp3\n",
      "Deleted file: Bhanware Ki Gunjan - Kal Aaj Aur Kal 128 Kbps.mp3\n",
      "Deleted file: Admi Jo Kahta Hai - Majboor 1974 128 Kbps.mp3\n",
      "Deleted file: Are Diwano Mujhe Pehchano - Don (1978) 128 Kbps.mp3\n",
      "Deleted file: Bhole O Bhole - Yaarana 128 Kbps.mp3\n",
      "Deleted file: Aisa Kabhie Hua Nahin - Yeh Vaada Raha 128 Kbps.mp3\n",
      "Deleted file: Aao Mere Yaro Aao - Rocky (1981) 128 Kbps.mp3\n",
      "Deleted file: Aashiq Hoon Baharon Ka - Aashiq Hoon Baharon Ka 128 Kbps.mp3\n",
      "Deleted file: Aa Chal Ke Tujhe - Door Ka Rahi Door Gagan Ki Chhaon Mein 128 Kbps.mp3\n",
      "Deleted file: Ae Haseeno Nazneeno - Chacha Zindabad 128 Kbps.mp3\n",
      "Deleted file: Aate Jate Khoobsurat Awara - Anurodh 128 Kbps.mp3\n",
      "Deleted file: Bahoot Khoobsoorat Ek Ladki - Dostana (1980) 128 Kbps.mp3\n",
      "Deleted file: Aai Khuda Har Faisla - Abdullah 128 Kbps.mp3\n",
      "Deleted file: Aanewala Pal Janewala Hai - Gol Maal 128 Kbps.mp3\n",
      "Standalone files have been deleted, folders are preserved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to your neural data folder\n",
    "neural_data_folder = '/home/avdhoot/third_sem/ds203/project/neural_data'  # Replace with your actual path\n",
    "\n",
    "# Loop through items in the neural data folder\n",
    "for item in os.listdir(neural_data_folder):\n",
    "    item_path = os.path.join(neural_data_folder, item)\n",
    "    \n",
    "    # Check if it's a standalone file (not a folder) and delete it\n",
    "    if os.path.isfile(item_path):\n",
    "        os.remove(item_path)\n",
    "        print(f\"Deleted file: {item}\")\n",
    "\n",
    "print(\"Standalone files have been deleted, folders are preserved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
